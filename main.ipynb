{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "from statistics import mode\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "import gc\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.lower()\n",
    "    num_word_to_digit = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
    "        'ten': '10'\n",
    "    }\n",
    "    for word, digit in num_word_to_digit.items():\n",
    "        text = text.replace(word, digit)\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "    contractions = {\n",
    "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
    "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
    "    }\n",
    "    for contraction, correct in contractions.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))  # Noneをフィルタリング\n",
    "    images, input_ids, attention_masks, answers, mode_answers = zip(*batch)\n",
    "\n",
    "    images = torch.stack(images)\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    max_len = max(len(a) for a in answers)\n",
    "    padded_answers = torch.zeros((len(answers), max_len), dtype=torch.long)\n",
    "    for i, a in enumerate(answers):\n",
    "        padded_answers[i, :len(a)] = a.clone().detach()\n",
    "\n",
    "    mode_answers = torch.stack(mode_answers)\n",
    "\n",
    "    return images, input_ids, attention_masks, padded_answers, mode_answers\n",
    "\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_path, image_dir, class_mapping, transform=None, answer=True):\n",
    "        self.transform = transform\n",
    "        self.image_dir = image_dir\n",
    "        self.df = pd.read_json(df_path)\n",
    "        self.answer = answer\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")  # CLIPプロセッサを追加\n",
    "\n",
    "        self.class_mapping = class_mapping\n",
    "        self.answer2idx = {row['answer']: row['class_id'] for _, row in class_mapping.iterrows()}\n",
    "        self.idx2answer = {row['class_id']: row['answer'] for _, row in class_mapping.iterrows()}\n",
    "\n",
    "        self.question2idx = {}\n",
    "        self.idx2question = {}\n",
    "\n",
    "        for question in self.df[\"w\"]:\n",
    "            question = process_text(question)\n",
    "            words = question.split(\" \")\n",
    "            for word in words:\n",
    "                if word not in self.question2idx:\n",
    "                    self.question2idx[word] = len(self.question2idx)\n",
    "        self.idx2question = {v: k for k, v in self.question2idx.items()}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            question_text = self.df[\"w\"][idx]\n",
    "            question_enc = self.tokenizer(question_text, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "            \n",
    "            input_ids = question_enc['input_ids'].squeeze()\n",
    "            attention_mask = question_enc['attention_mask'].squeeze()\n",
    "\n",
    "            if self.answer:\n",
    "                answers = [self.answer2idx.get(process_text(answer[\"answer\"]), -1) for answer in self.df[\"answers\"][idx]]\n",
    "                answers = [answer for answer in answers if answer != -1]\n",
    "                if not answers:\n",
    "                    return None  # マッピングにない回答がある場合はスキップ\n",
    "\n",
    "                mode_answer_idx = Counter(answers).most_common(1)[0][0]\n",
    "\n",
    "                return image, input_ids, attention_mask, torch.tensor(answers, dtype=torch.long), torch.tensor(mode_answer_idx, dtype=torch.long)\n",
    "            else:\n",
    "                return image, input_ids, attention_mask\n",
    "        except Exception as e:\n",
    "            return self.__getitem__((idx + 1) % len(self.df))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "# def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):\n",
    "#     total_acc = 0.\n",
    "\n",
    "#     for pred, answers in zip(batch_pred, batch_answers):\n",
    "#         acc = 0.\n",
    "#         pred = pred.item()\n",
    "#         num_match = 0\n",
    "#         for answer in answers:\n",
    "#             if pred == answer.item():\n",
    "#                 num_match += 1\n",
    "#         acc += min(num_match / 3, 1)\n",
    "#         total_acc += acc / 10\n",
    "\n",
    "#     return total_acc / len(batch_pred)\n",
    "def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):\n",
    "    total_acc = 0.\n",
    "\n",
    "    for pred, answers in zip(batch_pred, batch_answers):\n",
    "        acc = 0.\n",
    "        for i in range(len(answers)):\n",
    "            num_match = 0\n",
    "            for j in range(len(answers)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if pred == answers[j]:\n",
    "                    num_match += 1\n",
    "            acc += min(num_match / 3, 1)\n",
    "        total_acc += acc / 10\n",
    "\n",
    "    return total_acc / len(batch_pred)\n",
    "\n",
    "class MultiModalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(MultiModalAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=8)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, combined_features):\n",
    "        combined_features = combined_features.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        attn_output, _ = self.attention(combined_features, combined_features, combined_features)\n",
    "        return self.fc(attn_output.squeeze(1))\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, n_answer: int):\n",
    "        super(VQAModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.resnet = resnet152(weights=ResNet152_Weights.IMAGENET1K_V1)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 512)\n",
    "\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        # 各特徴量の次元を計算\n",
    "        resnet_feat_dim = 512\n",
    "        bert_feat_dim = 768\n",
    "        clip_vision_dim = 512\n",
    "        clip_text_dim = 512\n",
    "\n",
    "        hidden_size = resnet_feat_dim + bert_feat_dim + clip_vision_dim + clip_text_dim  # 2304\n",
    "        self.multi_modal_attention = MultiModalAttention(hidden_size=hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, n_answer)\n",
    "        )\n",
    "\n",
    "        self.residual = nn.Linear(hidden_size, n_answer)  # 残差接続用\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # ResNetで画像特徴を抽出\n",
    "        img_feat = self.resnet(image)  # [batch_size, 512]\n",
    "        \n",
    "        # BERTでテキスト特徴を抽出\n",
    "        question_feat = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output  # [batch_size, 768]\n",
    "\n",
    "        # CLIPで画像特徴を抽出\n",
    "        clip_img_feat = self.clip_model.get_image_features(pixel_values=image)  # [batch_size, 512]\n",
    "\n",
    "        # CLIPでテキスト特徴を抽出\n",
    "        text_inputs = self.clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)  # [batch_size, 512]\n",
    "\n",
    "        # 全ての特徴を結合\n",
    "        combined_feat = torch.cat([img_feat, question_feat, clip_img_feat, text_inputs], dim=1)  # [batch_size, 2304]\n",
    "\n",
    "        # 次元を確認するためのprint文を追加\n",
    "        # print(f\"img_feat: {img_feat.shape}\")\n",
    "        # print(f\"question_feat: {question_feat.shape}\")\n",
    "        # print(f\"clip_img_feat: {clip_img_feat.shape}\")\n",
    "        # print(f\"text_inputs: {text_inputs.shape}\")\n",
    "        # print(f\"combined_feat: {combined_feat.shape}\")\n",
    "\n",
    "        attention_output = self.multi_modal_attention(combined_feat)\n",
    "        output_fc = self.fc(attention_output)\n",
    "        output_residual = self.residual(combined_feat)\n",
    "        \n",
    "        return output_fc + output_residual  # 残差接続による出力の強化\n",
    "\n",
    "\n",
    "def soft_target(answers, n_classes, device):\n",
    "    target = torch.zeros(n_classes, device=device)\n",
    "    for answer in answers:\n",
    "        target[answer] += 1\n",
    "    target = target / target.sum()\n",
    "    return target\n",
    "\n",
    "class KLDivergenceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KLDivergenceLoss, self).__init__()\n",
    "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def forward(self, pred, answers, n_classes):\n",
    "        device = pred.device\n",
    "        soft_targets = torch.stack([soft_target(ans, n_classes, device) for ans in answers])\n",
    "        log_pred = F.log_softmax(pred, dim=1)\n",
    "        return self.kl_loss(log_pred, soft_targets)\n",
    "\n",
    "def train_eval(model, dataloader, optimizer, criterion, device, phase='train'):\n",
    "    if phase == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    simple_acc = 0\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    with torch.set_grad_enabled(phase == 'train'):\n",
    "        for batch in tqdm(dataloader, desc=f\"{phase.capitalize()}ing\"):\n",
    "            if batch is None:\n",
    "                continue\n",
    "            image, input_ids, attention_mask, answers, mode_answer = batch\n",
    "            image, input_ids, attention_mask, answers, mode_answer = \\\n",
    "                image.to(device), input_ids.to(device), attention_mask.to(device), answers.to(device), mode_answer.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                pred = model(image, input_ids, attention_mask)\n",
    "                loss = criterion(pred, answers, pred.size(1))\n",
    "\n",
    "            if phase == 'train':\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += VQA_criterion(pred.argmax(1), answers)\n",
    "            simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()\n",
    "\n",
    "            del image, input_ids, attention_mask, answers, mode_answer, pred, loss\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader)\n",
    "\n",
    "def train_and_visualize(model, n_epochs, optimizer, scheduler, dataloader_train, dataloader_valid, device):\n",
    "    criterion = KLDivergenceLoss()\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_accuracies = []\n",
    "\n",
    "    best_valid_acc = 0.0\n",
    "    best_valid_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in trange(n_epochs, desc=\"Epochs\"):\n",
    "        train_loss, train_acc, train_simple_acc = train_eval(model, dataloader_train, optimizer, criterion, device, phase='train')\n",
    "        valid_loss, valid_acc, valid_simple_acc = train_eval(model, dataloader_valid, optimizer, criterion, device, phase='eval')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracies.append(valid_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')\n",
    "\n",
    "        # 検証データの損失が最小のモデルを保存\n",
    "        # if valid_loss < best_valid_loss:\n",
    "        #     best_valid_loss = valid_loss\n",
    "        #     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        if valid_acc > best_valid_acc or (valid_acc == best_valid_acc and valid_loss < best_valid_loss):\n",
    "            best_valid_acc = valid_acc\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # 最良のモデルの重みをロード\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(valid_losses, label='Valid Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(valid_accuracies, label='Valid Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy Curve')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 交差検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "\n",
    "# データセットを訓練用と検証用に分割する関数\n",
    "def get_train_valid_loader(vqa_dataset, train_indices, valid_indices, batch_size=256):\n",
    "    train_dataset = Subset(vqa_dataset, train_indices)\n",
    "    valid_dataset = Subset(vqa_dataset, valid_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count(), pin_memory=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=os.cpu_count(), pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, valid_loader\n",
    "\n",
    "# 交差検証の実行\n",
    "def cross_validation_training(vqa_dataset, class_mapping_df, n_splits=5, num_epoch=15):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    models = []\n",
    "\n",
    "    for fold, (train_indices, valid_indices) in enumerate(kfold.split(vqa_dataset)):\n",
    "        print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "\n",
    "        train_loader, valid_loader = get_train_valid_loader(vqa_dataset, train_indices, valid_indices)\n",
    "\n",
    "        model = VQAModel(vocab_size=len(vqa_dataset.question2idx) + 1, n_answer=len(class_mapping_df)).to(device)\n",
    "\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epoch)\n",
    "\n",
    "        train_and_visualize(model, num_epoch, optimizer, scheduler, train_loader, valid_loader, device)\n",
    "\n",
    "        # 各フォールドの検証結果を保存\n",
    "        _, valid_acc, _ = train_eval(model, valid_loader, optimizer, KLDivergenceLoss(), device, phase='eval')\n",
    "        fold_results.append(valid_acc)\n",
    "\n",
    "        # モデルを保存\n",
    "        models.append(copy.deepcopy(model))\n",
    "\n",
    "        # メモリクリア\n",
    "        del model, optimizer, scheduler, train_loader, valid_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # 各フォールドの結果を表示\n",
    "    for fold, acc in enumerate(fold_results):\n",
    "        print(f\"Fold {fold + 1}: Valid Acc: {acc:.4f}\")\n",
    "\n",
    "    print(f\"Average Valid Acc: {np.mean(fold_results):.4f}\")\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定とデータセットの準備\n",
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class_mapping_df = pd.read_csv(\"/mnt/newdisk/data_annotations_class_mapping.csv\")\n",
    "vqa_dataset = VQADataset(df_path=\"/mnt/newdisk/train.json\", image_dir=\"/mnt/newdisk/train\", class_mapping=class_mapping_df, transform=transform)\n",
    "\n",
    "# 交差検証の実行\n",
    "models = cross_validation_training(vqa_dataset, class_mapping_df, n_splits=5, num_epoch=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestVQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_path, image_dir, class_mapping, transform=None):\n",
    "        self.df = pd.read_json(df_path)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        self.class_mapping = class_mapping\n",
    "        self.answer2idx = {row['answer']: row['class_id'] for _, row in class_mapping.iterrows()}\n",
    "        self.idx2answer = {row['class_id']: row['answer'] for _, row in class_mapping.iterrows()}\n",
    "\n",
    "        self.question2idx = {}\n",
    "        self.idx2question = {}\n",
    "\n",
    "        for question in self.df[\"question\"]:\n",
    "            question = process_text(question)\n",
    "            words = question.split(\" \")\n",
    "            for word in words:\n",
    "                if word not in self.question2idx:\n",
    "                    self.question2idx[word] = len(self.question2idx)\n",
    "        self.idx2question = {v: k for k, v in self.question2idx.items()}\n",
    "\n",
    "    def update_dict(self, dataset):\n",
    "        self.question2idx = dataset.question2idx\n",
    "        self.answer2idx = dataset.answer2idx\n",
    "        self.idx2question = dataset.idx2question\n",
    "        self.idx2answer = dataset.idx2answer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = f\"{self.image_dir}/{self.df.iloc[idx]['image']}\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        question_text = self.df[\"question\"][idx]\n",
    "        question_enc = self.tokenizer(question_text, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "        \n",
    "        input_ids = question_enc['input_ids'].squeeze()\n",
    "        attention_mask = question_enc['attention_mask'].squeeze()\n",
    "\n",
    "        return image, input_ids, attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions(models, test_loader, device, weights):\n",
    "    model_outputs = []\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        fold_submission = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Predicting\", unit=\"batch\"):\n",
    "                image, input_ids, attention_mask = batch\n",
    "                image, input_ids, attention_mask = image.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    pred = model(image, input_ids, attention_mask)\n",
    "                \n",
    "                fold_submission.append(pred.cpu().numpy())\n",
    "        \n",
    "        model_outputs.append(np.vstack(fold_submission))\n",
    "\n",
    "    weighted_preds = np.zeros_like(model_outputs[0])\n",
    "    for model_output, weight in zip(model_outputs, weights):\n",
    "        weighted_preds += model_output * weight\n",
    "\n",
    "    weighted_preds /= np.sum(weights)\n",
    "    return weighted_preds\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# テスト用データセットの作成\n",
    "test_dataset = TestVQADataset(df_path=\"/mnt/newdisk/valid.json\", image_dir=\"/mnt/newdisk/valid\", class_mapping=class_mapping_df, transform=test_transform)\n",
    "\n",
    "# test_datasetの辞書を更新\n",
    "test_dataset.update_dict(vqa_dataset)\n",
    "\n",
    "# データローダーを作成\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# アンサンブルモデルの重み\n",
    "weights = [0.2, 0.2, 0.2, 0.2, 0.2] \n",
    "\n",
    "# アンサンブル予測\n",
    "ensemble_preds = ensemble_predictions(models, test_loader, device, weights)\n",
    "\n",
    "# 予測結果の最も高いスコアのインデックスを取得\n",
    "pred_idx = np.argmax(ensemble_preds, axis=1)\n",
    "\n",
    "# 予測インデックスを回答に変換\n",
    "submission = [test_dataset.idx2answer[idx] for idx in pred_idx]\n",
    "\n",
    "# 予測結果を保存\n",
    "np.save(\"submission_ensemble_clip.npy\", np.array(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを保存するディレクトリ\n",
    "save_dir = \"/mnt/newdisk/clip_cross_valid_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# それぞれのモデルを保存\n",
    "for idx, model in enumerate(models):\n",
    "    model_path = os.path.join(save_dir, f\"model_fold_{idx}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
